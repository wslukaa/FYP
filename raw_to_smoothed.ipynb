{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ht_name:#venezuela ts_start:20180202-132400 isTrend:0\n",
      "ht_name:#venezuela ts_start:20180202-162600 isTrend:0\n",
      "ht_name:#venezuela ts_start:20180202-192800 isTrend:0\n",
      "ht_name:#venezuela ts_start:20180202-223000 isTrend:0\n",
      "ht_name:#jimin ts_start:20180202-132200 isTrend:0\n",
      "ht_name:#jimin ts_start:20180202-162400 isTrend:0\n",
      "ht_name:#jimin ts_start:20180202-202200 isTrend:0\n",
      "ht_name:#iheartawards ts_start:20180202-131600 isTrend:1\n",
      "ht_name:#iheartawards ts_start:20180202-161800 isTrend:0\n",
      "ht_name:#iheartawards ts_start:20180202-192000 isTrend:0\n",
      "ht_name:#iheartawards ts_start:20180202-222200 isTrend:0\n",
      "ht_name:#iheartawards ts_start:20180203-012400 isTrend:0\n",
      "ht_name:#iheartawards ts_start:20180203-042600 isTrend:0\n",
      "ht_name:#iheartawards ts_start:20180203-072800 isTrend:0\n",
      "ht_name:#bestfanarmy ts_start:20180202-132800 isTrend:0\n",
      "ht_name:#bestfanarmy ts_start:20180202-163000 isTrend:0\n",
      "ht_name:#btsarmy ts_start:20180202-133000 isTrend:0\n",
      "ht_name:#btsarmy ts_start:20180202-163200 isTrend:0\n",
      "ht_name:#liar ts_start:20180202-174000 isTrend:0\n",
      "ht_name:#seungyoon ts_start:20180203-031600 isTrend:1\n",
      "ht_name:#seungyoon ts_start:20180203-061800 isTrend:0\n",
      "ht_name:#stepup ts_start:20180202-174000 isTrend:0\n"
     ]
    }
   ],
   "source": [
    "#hts_ts_list\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = 'data/testing-folder-with-5-tweet-files/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "min = 2\n",
    "trend_multiple = 3\n",
    "\n",
    "def normalize(df):\n",
    "    # 180/2 = 90\n",
    "    return (df+1) / (df + 1).rolling(90).mean()\n",
    "\n",
    "#Emphasize large spikes\n",
    "def emphasize(df):\n",
    "    #rolling 2\n",
    "    alpha = 1.2\n",
    "    em = lambda x: ( np.abs(x[0] - x[1])**alpha )\n",
    "    df1 = df.rolling(2).apply(em)\n",
    "    return df1\n",
    "\n",
    "def smooth(df):\n",
    "    # 10 min rolling\n",
    "    r = 5\n",
    "    return df.rolling(r).sum()\n",
    "\n",
    "def detect_valid_trend(df, df_raw):\n",
    "    threshold = 2\n",
    "    df1 =  ( (df / df.shift().rolling(90).mean() ) > trend_multiple )*1\n",
    "    no_consec = lambda x: ( 0 if sum(x) > 1 else sum(x))\n",
    "    df_trend = df1\n",
    "    #df_trend = df1.rolling(90).apply(no_consec)\n",
    "    df_valid = ( (df_raw.shift().rolling(90).sum() > 200) & (df_raw.shift().rolling(90).mean() > threshold)  )*1\n",
    "    df_trend = df_trend * df_valid\n",
    "    return df_valid, df_trend\n",
    "\n",
    "def count_hts():\n",
    "    hts = {}\n",
    "    for fpath in onlyfiles:\n",
    "        with open(mypath+fpath) as f:\n",
    "            for line in f:\n",
    "                tweet = json.loads(line)\n",
    "                for ht in tweet['hashtags']:\n",
    "                    ht_name = ht['text'].lower()\n",
    "                    if ht_name in hts:\n",
    "                        hts[ht_name] += 1\n",
    "                    else:\n",
    "                        hts[ht_name] = 1\n",
    "    hts = sorted(hts.items(), key=lambda x: x[1], reverse=True)\n",
    "    hts_out = {}\n",
    "    for x in hts:\n",
    "        if x[1] < 200:\n",
    "            break\n",
    "        hts_out[x[0]] = x[1]\n",
    "    return hts_out\n",
    "\n",
    "def get_ts_from_hts(hts):\n",
    "    hts_ts_list = {}\n",
    "    for fpath in onlyfiles:\n",
    "        with open(mypath+fpath) as inputF:\n",
    "            #print(fpath)\n",
    "            for line_index, line in enumerate(inputF):\n",
    "                tweet = json.loads(line)\n",
    "                for ht in tweet['hashtags']:\n",
    "                    ht_name = ht['text'].lower()\n",
    "                    if ht_name not in hts:\n",
    "                        continue\n",
    "                    if ht_name not in hts_ts_list:\n",
    "                        hts_ts_list[ht_name] = []\n",
    "                    tweet_time = datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    tt = tweet_time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    hts_ts_list[ht_name].append(tt)\n",
    "    return hts_ts_list\n",
    "    '''with open('hts_ts_list_new','a') as f:\n",
    "        for k,v in hts_ts_list.items():\n",
    "            f.write(json.dumps([k,v])+'\\n')'''\n",
    "\n",
    "def get_hts_3h_samples2(d):\n",
    "    o_list = []\n",
    "    ht_name = '#' + d[0]\n",
    "    timestamp_list = [ datetime.strptime(x, \"%Y%m%d-%H%M%S\") for x in d[1] ]\n",
    "    freq = str(min) + 'min'\n",
    "    # 148 days, from/includinig sep25, to/not including feb 20\n",
    "    rng = pd.date_range('25/09/2017', periods=148*24*60/min, freq=freq)\n",
    "    df = pd.DataFrame(index=rng)\n",
    "    kw = {ht_name:pd.Series(np.zeros(len(rng)),dtype=np.int, index=rng)}\n",
    "    df = df.assign(**kw)\n",
    "    \n",
    "    i = 0\n",
    "    for ts in timestamp_list:\n",
    "        while rng[i] < ts:\n",
    "            i += 1\n",
    "        i -= 1\n",
    "        df.at[rng[i],ht_name] += 1\n",
    "        '''for i in range(0,len(rng)-1):\n",
    "            if ts >= rng[i] and ts < rng[i+1]:\n",
    "                df.at[rng[i],ht_name] += 1'''\n",
    "    \n",
    "    df0 = df\n",
    "    df1 = normalize(df0)\n",
    "    df2 = emphasize(df1)\n",
    "    df3 = smooth(df2)\n",
    "    df_valid, df_trend = detect_valid_trend(df3, df0)\n",
    "    df_nonTrend = df_valid\n",
    "    tag_name = ht_name\n",
    "    \n",
    "    num_output = 0\n",
    "    for i in range(90, len(rng)-90):\n",
    "        # trend first\n",
    "        #if sum(df_valid[ht_name].tolist()[i-90:i-1]) == 0 and df_valid.at[rng[i],ht_name]:\n",
    "        if df_valid.at[rng[i],ht_name]:\n",
    "            isTrend = 0 \n",
    "            if df_trend.at[rng[i],ht_name] == 1 and sum(df_trend[ht_name].tolist()[i-90:i-1]) == 0:\n",
    "                isTrend = 1\n",
    "                for j in range(i+1,i+90):\n",
    "                    df_trend.at[rng[j],ht_name] = 0\n",
    "                    df_valid.at[rng[j],ht_name] = 0\n",
    "            elif df_nonTrend.at[rng[i],ht_name] == 1 and sum(df_nonTrend[ht_name].tolist()[i-90:i-1]) == 0:\n",
    "                isTrend = 0\n",
    "                for j in range(i+1,i+90):\n",
    "                    df_nonTrend.at[rng[j],ht_name] = 0\n",
    "                    df_valid.at[rng[j],ht_name] = 0\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            num_output += 1\n",
    "                \n",
    "            #df0 = df[start_dt:start_dt+timedelta(minutes=180)]\n",
    "            df_raw = df0[rng[i-90]:rng[i]]\n",
    "            df_nor = df1[rng[i-90]:rng[i]]\n",
    "            df_em = df2[rng[i-90]:rng[i]]\n",
    "            df_s = df3[rng[i-90]:rng[i]]\n",
    "            \n",
    "            obj = {\n",
    "                        'ht_name':ht_name,\n",
    "                        'ts_start':rng[i-90].strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                        'isTrend':isTrend,\n",
    "                        'ts_full':[x.strftime(\"%Y%m%d-%H%M%S\") for x in rng[i-90:i] ],\n",
    "                        'df_raw':df_raw[ht_name].tolist(),\n",
    "                        'df_nor':df_nor[ht_name].tolist(),\n",
    "                        'df_em':df_em[ht_name].tolist(),\n",
    "                        'df_smoothed':df_s[ht_name].tolist() ,\n",
    "            }\n",
    "            o_list.append(obj)\n",
    "            '''with open('raw_to_smoothed_hts.txt','a') as outF:\n",
    "                outF.write(json.dumps(o_list) + '\\n')'''\n",
    "                \n",
    "    return o_list\n",
    "\n",
    "def get_hts_3h_samples(hts_ts_list):\n",
    "    hts_3h_samples = []\n",
    "    for k,v in hts_ts_list.items():\n",
    "        d = [k, v]\n",
    "        o_list = get_hts_3h_samples2(d)\n",
    "        for obj in o_list:\n",
    "            hts_3h_samples.append(obj)\n",
    "    return hts_3h_samples\n",
    "        \n",
    "def get_ts_from_cluster(cluster_raw):\n",
    "    # cluster_raw = [[\" daily\", \" thanks\", \" latest\"], [\" nsfw\", \" dating\", \" adultwebcamdate\"] ]\n",
    "    word_to_cluster = {}\n",
    "    cluster = {}\n",
    "    \n",
    "    for c in cluster_raw:\n",
    "        c0 = c[0].strip()\n",
    "        c1 = c[1].strip()\n",
    "        c2 = c[2].strip()\n",
    "        cluster_id = c0 + c1 + c2\n",
    "        word_to_cluster[c0] = cluster_id\n",
    "        word_to_cluster[c1] = cluster_id\n",
    "        word_to_cluster[c2] = cluster_id\n",
    "        cluster[cluster_id] = {}\n",
    "        cluster[cluster_id]['words'] = [c0, c1, c2]\n",
    "    \n",
    "    cluster_ts_list = {}\n",
    "    for fpath in onlyfiles:\n",
    "        with open(mypath+fpath) as inputF:\n",
    "            for line_index, line in enumerate(inputF):\n",
    "                tweet = json.loads(line)\n",
    "                words_in_text = tweet['text'].lower().split()\n",
    "                for w in words_in_text:\n",
    "                    if w not in word_to_cluster:\n",
    "                        continue\n",
    "                    cluster_id = word_to_cluster[w]\n",
    "                    if cluster_id not in cluster_ts_list:\n",
    "                        cluster_ts_list[cluster_id] = {}\n",
    "                        cluster_ts_list[cluster_id]['words'] = cluster[cluster_id]['words']\n",
    "                        cluster_ts_list[cluster_id]['ts'] = []\n",
    "                    tweet_time = datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    tt = tweet_time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    cluster_ts_list[cluster_id]['ts'].append(tt)\n",
    "                    break\n",
    "       \n",
    "    return cluster_ts_list\n",
    "    '''with open('clusterfyp_withpca_optimised_ts_list_new','a') as f:\n",
    "        for k,v in cluster_ts_list.items():\n",
    "            f.write(json.dumps([k,v['words'],v['ts']])+'\\n')'''\n",
    "    \n",
    "def get_cluster_3h_samples2(d):\n",
    "    o_list = []\n",
    "    cluster_id = '#' + d[0]\n",
    "    words = d[1]\n",
    "    timestamp_list = [ datetime.strptime(x, \"%Y%m%d-%H%M%S\") for x in d[2] ]\n",
    "    freq = str(min) + 'min'\n",
    "    # 148 days, from/includinig sep25, to/not including feb 20\n",
    "    rng = pd.date_range('25/09/2017', periods=148*24*60/min, freq=freq)\n",
    "    df = pd.DataFrame(index=rng)\n",
    "    kw = {cluster_id:pd.Series(np.zeros(len(rng)),dtype=np.int, index=rng)}\n",
    "    df = df.assign(**kw)\n",
    "    \n",
    "    i = 0\n",
    "    for ts in timestamp_list:\n",
    "        while rng[i] < ts:\n",
    "            i += 1\n",
    "        i -= 1\n",
    "        df.at[rng[i],cluster_id] += 1\n",
    "        \n",
    "    df0 = df\n",
    "    df1 = normalize(df0)\n",
    "    df2 = emphasize(df1)\n",
    "    df3 = smooth(df2)\n",
    "    df_valid, df_trend = detect_valid_trend(df3, df0)\n",
    "    df_nonTrend = df_valid\n",
    "    tag_name = cluster_id\n",
    "    \n",
    "    num_output = 0\n",
    "    for i in range(90, len(rng)-90):\n",
    "        # trend first\n",
    "        #if sum(df_valid[ht_name].tolist()[i-90:i-1]) == 0 and df_valid.at[rng[i],ht_name]:\n",
    "        if df_valid.at[rng[i],cluster_id]:\n",
    "            isTrend = 0 \n",
    "            if df_trend.at[rng[i],cluster_id] == 1 and sum(df_trend[cluster_id].tolist()[i-90:i-1]) == 0:\n",
    "                isTrend = 1\n",
    "                for j in range(i+1,i+90):\n",
    "                    df_trend.at[rng[j],cluster_id] = 0\n",
    "                    df_valid.at[rng[j],cluster_id] = 0\n",
    "            elif df_nonTrend.at[rng[i],cluster_id] == 1 and sum(df_nonTrend[cluster_id].tolist()[i-90:i-1]) == 0:\n",
    "                isTrend = 0\n",
    "                for j in range(i+1,i+90):\n",
    "                    df_nonTrend.at[rng[j],cluster_id] = 0\n",
    "                    df_valid.at[rng[j],cluster_id] = 0\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            num_output += 1\n",
    "                \n",
    "            #df0 = df[start_dt:start_dt+timedelta(minutes=180)]\n",
    "            df_raw = df0[rng[i-90]:rng[i]]\n",
    "            df_nor = df1[rng[i-90]:rng[i]]\n",
    "            df_em = df2[rng[i-90]:rng[i]]\n",
    "            df_s = df3[rng[i-90]:rng[i]]\n",
    "            \n",
    "            obj = {\n",
    "                        'cluster_id':cluster_id,\n",
    "                        'cluster_words':words,\n",
    "                        'ts_start':rng[i-90].strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                        'isTrend':isTrend,\n",
    "                        'ts_full':[x.strftime(\"%Y%m%d-%H%M%S\") for x in rng[i-90:i] ],\n",
    "                        'df_raw':df_raw[cluster_id].tolist(),\n",
    "                        'df_nor':df_nor[cluster_id].tolist(),\n",
    "                        'df_em':df_em[cluster_id].tolist(),\n",
    "                        'df_smoothed':df_s[cluster_id].tolist() ,\n",
    "            }\n",
    "            o_list.append(obj)\n",
    "            '''with open('clusterfyp_withpca_optimised_raw_plus_preprocessed_multiple_'+str(trend_multiple),'a') as outF:\n",
    "                outF.write(json.dumps(o_list) + '\\n')'''\n",
    "\n",
    "    return o_list\n",
    "\n",
    "def get_cluster_3h_samples(cluster_ts_list):\n",
    "    cluster_3h_samples = []\n",
    "    for k,v in cluster_ts_list.items():\n",
    "        d = [k, v['words'], v['ts']]\n",
    "        o_list = get_cluster_3h_samples2(d)\n",
    "        for obj in o_list:\n",
    "            cluster_3h_samples.append(obj)\n",
    "    return cluster_3h_samples\n",
    "\n",
    "def main():\n",
    "    hts_filtered = count_hts()\n",
    "    hts_ts_list = get_ts_from_hts(hts_filtered)\n",
    "    hts_3h_samples = get_hts_3h_samples(hts_ts_list)\n",
    "    for s in hts_3h_samples:\n",
    "        print('ht_name:{} ts_start:{} isTrend:{}'.format(s['ht_name'],s['ts_start'],s['isTrend']))\n",
    "    \n",
    "    # Winson need to make the cluster\n",
    "    #cluster = None\n",
    "    #cluster_ts_list = get_ts_from_cluster(cluster)\n",
    "    #cluster_3h_samples = get_cluster_3h_samples(cluster_ts_list)\n",
    "                    \n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
